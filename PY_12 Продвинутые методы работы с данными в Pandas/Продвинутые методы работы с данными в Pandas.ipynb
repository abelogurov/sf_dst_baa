{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВСПОМНИМ, С КАКИМИ ДАННЫМИ МЫ РАБОТАЕМ\n",
    "\n",
    "# В этом модуле мы продолжим нашу работу с датасетом о продажах объектов недвижимости в Мельбурне и его пригородах. \n",
    "# Информацию об исходных данных вы можете найти здесь.\n",
    "\n",
    "# В прошлом модуле мы совершили множество преобразований над нашей таблицей — давайте вспомним их:\n",
    "\n",
    "# удалили столбцы index и Coordinates;\n",
    "# создали признак средней площади одной комнаты MeanRoomsSquare \n",
    "# и ввели коэффициент соотношения площади здания к площади участка — AreaRatio;\n",
    "# преобразовали признак даты продажи Date в формат datetime и создали на его основе следующие столбцы: \n",
    "# номер месяц продажи (MonthSale), номер дня недели продажи (WeekdaySale), «признак-мигалку» выходного дня (Weekend);\n",
    "# заменили признак года постройки здания YearBuilt на его возраст AgeBuilding;\n",
    "# извлекли из признака адреса объекта Address новый признак подтипа улицы StreetType и удалили столбец с адресом;\n",
    "# уменьшили число уникальных наименований агентств по недвижимости (SellerG), а также число пригородов (Suburb);\n",
    "# выделили категориальные признаки и преобразовали их в тип данных сategory;\n",
    "# заменили сокращённые названия категорий признака типа объекта Type на их полные названия (h — house, t —  townhouse, u — unit).\n",
    "\n",
    "# Прочитаем датасет и выведем первые пять его строк:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "melb_df = pd.read_csv('data/melb_data_fe.csv')\n",
    "melb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Файл подгрузился верно, однако есть один нюанс. Давайте посмотрим на информацию о столбцах с помощью метода info():\n",
    "\n",
    "print(melb_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЗАДАНИЕ 1.1\n",
    "# Преобразуйте столбец Date в формат datetime и выделите квартал (quarter) продажи объектов недвижимости. \n",
    "# Найдите второй по популярности квартал продажи. В качестве ответа запишите число объектов, проданных в этом квартале.\n",
    "melb_df['Date'] = pd.to_datetime(melb_df['Date'])\n",
    "quarters = melb_df['Date'].dt.quarter\n",
    "print(quarters.value_counts().iloc[1])\n",
    "\n",
    "# ЗАДАНИЕ 1.2\n",
    "# Преобразуйте все столбцы, в которых меньше 150 уникальных значений, в тип данных category, \n",
    "# исключив из преобразования столбцы Date, Rooms, Bedroom, Bathroom, Car.\n",
    "# В качестве ответа запишите результирующее количество столбцов, которые имеют тип данных category.\n",
    "cols_to_exclude = ['Date', 'Rooms', 'Bedroom', 'Bathroom', 'Car'] \n",
    "max_unique_count = 150 \n",
    "for col in melb_df.columns: \n",
    "    if melb_df[col].nunique() < max_unique_count and col not in cols_to_exclude: \n",
    "        melb_df[col] = melb_df[col].astype('category')\n",
    "print(melb_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Сортировка данных в DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавить страницу в мои закладки\n",
    "# Часто бывает так, что таблицу или промежуточный результат операций с ней необходимо отсортировать по какому-то критерию. \n",
    "# Например, для отчётности вам необходимо предоставить список проданных объектов недвижимости, отсортированный по возрастанию цены \n",
    "# или дате продажи. Рассмотрим основные подходы к решению таких задач.\n",
    "\n",
    "# МЕТОД SORT_VALUES()\n",
    "# Для сортировки значений в DataFrame по значениям одного или нескольких столбцов используется метод sort_values().\n",
    "# Основные параметры метода sort_values()\n",
    "\n",
    "# СОРТИРОВКА ПО ЗНАЧЕНИЯМ ОДНОГО СТОЛБЦА\n",
    "# Приведём несколько примеров сортировки нашей таблицы с недвижимостью.\n",
    "# Отсортируем таблицу по возрастанию цены объектов недвижимости (Price):\n",
    "melb_df.sort_values(by='Price').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Мы вывели десять строк таблицы, чтобы убедиться в верном порядке сортировки. \n",
    "# Также обратите внимание на индексы таблицы — их значения сохранились из исходной таблицы.\n",
    "\n",
    "# А теперь отсортируем таблицу по убыванию (от самой последней до самой первой) даты продажи объекта (Date). \n",
    "# Для этого выставим параметр ascending на False:\n",
    "melb_df.sort_values(by='Date', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СОРТИРОВКА ПО ЗНАЧЕНИЯМ НЕСКОЛЬКИХ СТОЛБЦОВ\n",
    "\n",
    "# Для сортировки по значениям нескольких столбцов необходимо передать названия этих столбцов в параметр by в виде списка. \n",
    "# При этом важно обращать внимание на порядок следования столбцов.\n",
    "\n",
    "# Так, например, отсортируем таблицу сначала по возрастанию расстояния от центра города (Distance), \n",
    "# а затем — по возрастанию цены объекта (Price). Для того чтобы вывод был более наглядным, выделим каждую десятую строку \n",
    "# из столбцов Distance и Price результирующей таблицы:\n",
    "melb_df.sort_values(by=['Distance', 'Price']).loc[::10, ['Distance', 'Price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# КОМБИНИРОВАНИЕ СОРТИРОВКИ С ФИЛЬТРАЦИЕЙ\n",
    "\n",
    "# А теперь рассмотрим применение сортировки на практике.\n",
    "\n",
    "# Предположим, компания McGrath поручила нам восстановить хронологию продаж таунхаусов, \n",
    "# у которых площадь участка существенно больше площади здания, чтобы понять, \n",
    "# как часто компания справляется с таким сложным видом объектов. \n",
    "# Объекты, проданные в один и тот же день, мы бы хотели сортировать по значению коэффициента соотношения площадей. \n",
    "\n",
    "# Найдём информацию о таунхаусах (Type), проданных компанией (SellerG) McGrath, у которых коэффициент соотношения площадей \n",
    "# здания и участка (AreaRatio) меньше -0.8. Результат отсортируем по дате продажи (Date) в порядке возрастания, \n",
    "# а после проведём сортировку по убыванию коэффициента соотношения площадей. Также обновим старые индексы на новые, \n",
    "# установив параметр ignore_index на True. Для наглядности результата выберем из таблицы только столбцы Data и AreaRatio:\n",
    "\n",
    "mask1 = melb_df['AreaRatio'] < -0.8\n",
    "mask2 = melb_df['Type'] == 'townhouse'\n",
    "mask3 = melb_df['SellerG'] == 'McGrath'\n",
    "melb_df[mask1 & mask2 & mask3].sort_values(\n",
    "    by=['Date', 'AreaRatio'],\n",
    "    ascending=[True, False],\n",
    "    ignore_index=True\n",
    ").loc[:, ['Date', 'AreaRatio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЗАДАНИЕ 2.2\n",
    "# Произведите сортировку столбца AreaRatio по убыванию. При этом индексы полученной таблицы замените на новые. \n",
    "# Какое значение площади здания находится в строке 1558? Ответ округлите до целого числа.\n",
    "int(melb_df.sort_values(\n",
    "    by='AreaRatio', \n",
    "    ignore_index=True,\n",
    "    ascending=False\n",
    ").loc[1558, 'BuildingArea'])\n",
    "\n",
    "# ЗАДАНИЕ 2.3\n",
    "# Найдите таунхаусы (Type) с количеством жилых комнат (Rooms) больше 2. \n",
    "# Отсортируйте полученную таблицу сначала по возрастанию числа комнат, а затем по убыванию средней площади комнат (MeanRoomsSquare). \n",
    "# Индексы таблицы замените на новые. Какая цена будет у объекта в строке 18? Ответ запишите в виде целого числа.\n",
    "mask1 = melb_df['Type'] == 'townhouse'\n",
    "mask2 = melb_df['Rooms'] > 2\n",
    "int(melb_df[mask1&mask2].sort_values(\n",
    "    by=['Rooms', 'MeanRoomsSquare'],\n",
    "    ascending=[True, False],\n",
    "    ignore_index=True\n",
    ").loc[18, 'Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Группировка данных в DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Одна из основных задач анализа данных — это группировка данных и сравнение показателей в группах. Например, \n",
    "# нам необходимо сравнить средний уровень заработной платы в зависимости от пола/уровня образования. \n",
    "# Или же мы хотим проследить, какая группа клиентов приносит нам наибольший доход, чтобы направить своё внимание на эту группу.\n",
    "\n",
    "# В некоторых случаях группировки может быть достаточно, чтобы ответить на вопросы бизнеса. \n",
    "# В других случаях это может стать первым шагом в более сложном анализе. \n",
    "# Так, например, благодаря группировке мы можем выявлять признаки, которые не несут статистической значимости, или признаки, \n",
    "# которые вносят наибольший вклад. \n",
    "\n",
    "# Так или иначе, владение группировкой — важный навык, который открывает новые возможности по работе с данными.\n",
    "\n",
    "# МЕТОД GROUPBY()\n",
    "\n",
    "# В библиотеке Pandas для группировки данных по одному или нескольким признакам можно использовать метод groupby().\n",
    "\n",
    "# Основные параметры метода groupby()\n",
    "# Метод groupby() возвращает объект DataFrameGroupBy, который хранит в себе информацию о том, \n",
    "# какие строки относятся к определённой группе, и сам по себе не представляет для нас интереса. \n",
    "\n",
    "# Однако к этому объекту можно применять уже знакомые нам агрегирующие методы (mean, median, sum и т. д.), \n",
    "# чтобы рассчитывать показатели внутри каждой группы.\n",
    "\n",
    "# ГРУППИРОВКА ДАННЫХ ПО ОДНОМУ КРИТЕРИЮ С ОДНОЙ АГРЕГАЦИЕЙ\n",
    "\n",
    "# Рассмотрим группировку данных на примере нашей таблицы с недвижимостью.\n",
    "\n",
    "# Применим агрегирующую функцию среднего к результату работы groupby(). \n",
    "# В качестве столбца для группировки возьмём столбец типа объекта недвижимости (Type):\n",
    "melb_df.groupby(by='Type').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как правило, нам не нужна информация обо всех столбцах, \n",
    "# поэтому агрегирующие методы можно применять только к интересующему нас столбцу. \n",
    "# Например, давайте сравним средние цены на объекты в зависимости от их типа:\n",
    "melb_df.groupby('Type')['Price'].mean()\n",
    "\n",
    "# Примечание. Обратите внимание, что, так как мы считаем только один показатель (среднее) для одного столбца, \n",
    "# в результате мы получаем объект Series.\n",
    "# Из этой маленькой таблицы видно, что наибольшей средней ценой обладают объекты типа house (дома, коттеджи, виллы). \n",
    "# Следовательно, можно сделать вывод, что тип постройки является значимым фактором при определении цены объекта недвижимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь давайте выясним, какие регионы (Regionname) наиболее удалены от центра Мельбурна.\n",
    "# Для этого найдём минимальное значение расстояния от центра города до объекта в зависимости от его региона. \n",
    "# Результат отсортируем по убыванию расстояния:\n",
    "melb_df.groupby('Regionname')['Distance'].min().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ГРУППИРОВКА ДАННЫХ ПО ОДНОМУ КРИТЕРИЮ С НЕСКОЛЬКИМИ АГРЕГАЦИЯМИ\n",
    "\n",
    "# Чтобы рассчитать несколько агрегирующих методов, можно воспользоваться методом agg(), \n",
    "# который принимает список строк с названиями агрегаций.\n",
    "\n",
    "# Давайте построим таблицу для анализа продаж по месяцам. Для этого найдём количество продаж, \n",
    "# а также среднее и максимальное значения цен объектов недвижимости (Price), сгруппированных по номеру месяца продажи (MonthSale). \n",
    "# Результат отсортируем по количеству продаж в порядке убывания:\n",
    "melb_df.groupby('MonthSale')['Price'].agg(\n",
    "    ['count', 'mean', 'max']\n",
    ").sort_values(by='count', ascending=False)\n",
    "\n",
    "# Примечание. Обратите внимание, что, так как мы считаем несколько показателей для одного столбца, \n",
    "# в результате мы получаем объект DataFrame.\n",
    "\n",
    "# В результате применения метода agg(), в который мы передали список с названиями интересующих нас агрегирующих функций, \n",
    "# мы получаем DataFrame со столбцами count, mean и max, где для каждого месяца рассчитаны соответствующие параметры. \n",
    "# Результат сортируем по столбцу count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примечание. Если вам нужна полная информация обо всех основных статистических характеристиках внутри каждой группы, \n",
    "# вы можете воспользоваться методом agg(), передав в качестве его параметра строку 'describe':\n",
    "melb_df.groupby('MonthSale')['Price'].agg('describe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# После базовых математических функций наиболее частым агрегированием является подсчёт числа уникальных значений. \n",
    "# Так, например, мы можем вычислить число уникальных риелторских компаний в зависимости от региона, чтобы понять, \n",
    "# в каких регионах конкуренция на рынке недвижимости меньше. Это можно сделать, передав в параметр метода agg() строку 'nunique'. \n",
    "\n",
    "# Более того, метод agg() поддерживает использование и других функций. Передадим дополнительно встроенную функцию set, \n",
    "# чтобы получить множество из агентств недвижимости, которые работают в каждом из регионов:\n",
    "melb_df.groupby('Regionname')['SellerG'].agg(\n",
    "    \t\t['nunique', set]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЗАДАНИЕ 3.1\n",
    "# Сгруппируйте данные по признаку количества комнат и найдите среднюю цену объектов недвижимости в каждой группе. \n",
    "# В качестве ответа запишите количество комнат, для которых средняя цена наибольшая.\n",
    "melb_df.groupby('Rooms')['Price'].mean().sort_values(ascending=False)\n",
    "\n",
    "# ЗАДАНИЕ 3.2\n",
    "# Какой регион имеет наименьшую протяжённость по географической широте (Lattitude)?\n",
    "# Для ответа на этот вопрос рассчитайте стандартное отклонение широты для каждого региона.\n",
    "# В качестве ответа запишите название этого региона.\n",
    "melb_df.groupby('Regionname')['Lattitude'].std().sort_values()\n",
    "\n",
    "# ЗАДАНИЕ 3.3\n",
    "# Какая риелторская компания (SellerG) имеет наименьшую общую выручку за период с 1 мая по 1 сентября (включительно) 2017 года?\n",
    "# Для ответа на этот вопрос рассчитайте сумму продаж (Price) каждой компании в заданный период.\n",
    "# Не забудьте перевести даты в формат datetime.\n",
    "date1 = pd.to_datetime('2017-05-01')\n",
    "date2 = pd.to_datetime('2017-09-01')\n",
    "mask = (date1 <= melb_df['Date']) & (melb_df['Date'] <= date2)\n",
    "melb_df[mask].groupby('SellerG')['Price'].sum().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Сводные таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сводные таблицы — это распространённый инструмент для агрегации данных.\n",
    "\n",
    "# Сводная таблица принимает на вход данные из отдельных столбцов и группирует их. \n",
    "# В результате получается новая таблица, которая позволяет увидеть многомерное обобщение данных. \n",
    "# Таким образом, благодаря сводным таблицам мы можем оценить зависимость между двумя и более признаками данных.\n",
    "\n",
    "# Мы чаще сталкиваемся со сводными таблицами, чем с обычными, в плоском виде, так как сводные таблицы удобнее для анализа \n",
    "# и быстрых выводов, а также позволяют увидеть более общие зависимости между признаками, нежели простая группировка данных.\n",
    "\n",
    "# Инструмент сводных таблиц также широко популярен среди тех, кто использует Excel или какие-либо BI-системы.\n",
    "\n",
    "# МЕТОД GROUPBY КАК СПОСОБ ПОСТРОЕНИЯ СВОДНЫХ ТАБЛИЦ\n",
    "\n",
    "# На самом деле мы с вами уже строили простейшие одномерные сводные таблицы с помощью метода groupby — мы рассматривали \n",
    "# сводную таблицу в контексте группировки по одному признаку. \n",
    "\n",
    "# Например, мы уже умеем строить таблицу, которая показывает зависимость медианной цены и площади здания от числа комнат:\n",
    "melb_df.groupby('Rooms')[['Price', 'BuildingArea']].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Также можно построить таблицу, в которой мы будем учитывать не только число комнат, но и тип здания (Type). \n",
    "# Для этого в параметрах метода groupby() укажем список из нескольких интересующих нас столбцов.\n",
    "melb_df.groupby(['Rooms', 'Type'])['Price'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В результате выполнения такого кода мы получаем Series, которая обладает несколькими уровнями индексов: \n",
    "# первый уровень — число комнат, второй уровень — тип здания. Такая организация индексов называется иерархической. \n",
    "# Вычисление параметра (средней цены) происходит во всех возможных комбинациях признаков.\n",
    "\n",
    "# Для того, чтобы финальный результат был представлен в виде сводной таблицы \n",
    "# (первый группировочный признак по строкам, а второй — по столбцам), а не в виде Series с иерархическими индексами, \n",
    "# к результату чаще всего применяют метод unstack(), который позволяет переопределить вложенный индекс в виде столбцов таблицы:\n",
    "melb_df.groupby(['Rooms', 'Type'])['Price'].mean().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# МЕТОД PIVOT_TABLE ДЛЯ ПОСТРОЕНИЯ СВОДНЫХ ТАБЛИЦ\n",
    "\n",
    "# На самом деле метод groupby редко используется при двух параметрах, \n",
    "# так как для построения сводных таблиц существует специальный и более простой метод — pivot_table().\n",
    "\n",
    "# Основные параметры метода pivot_table()\n",
    "# Давайте построим ту же самую таблицу, но уже с использованием метода pivot_table. \n",
    "# В качестве параметра values укажем столбец Price, в качестве индексов сводной таблицы возьмём Rooms, \n",
    "# а в качестве столбцов — Type. Агрегирующую функцию оставим по умолчанию (среднее). \n",
    "# Дополнительно заменим пропуски в таблице на значение 0. Финальный результат \n",
    "# для наглядности вывода округлим с помощью метода round() до целых.\n",
    "melb_df.pivot_table(\n",
    "    values='Price',\n",
    "    index='Rooms',\n",
    "    columns='Type',\n",
    "    fill_value=0\n",
    ").round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Несложно понять, что метод pivot_table() имеет преимущество перед группировкой по нескольким критериям. \n",
    "# Оно заключается в наличии специальных аргументов для строк и столбцов сводной таблицы, \n",
    "# благодаря чему уменьшается вероятность запутаться при построении более сложных (многомерных) сводных таблиц,\n",
    "# о которых мы поговорим далее.\n",
    "\n",
    "# А теперь давайте проанализируем продажи в каждом из регионов в зависимости от того, будний был день или выходной. \n",
    "# Для этого построим сводную таблицу, в которой строками будут являться названия регионов (Regionname), \n",
    "# а в столбцах будет располагаться наш «признак-мигалка» выходного дня (Weekend), который равен 1, \n",
    "# если день был выходным, и 0 — в противном случае. В качестве значений сводной таблицы возьмём количество продаж.\n",
    "melb_df.pivot_table(\n",
    "    values='Price',\n",
    "    index='Regionname',\n",
    "    columns='Weekend',\n",
    "    aggfunc='count',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разберём ещё один пример: найдём, как зависит средняя и медианная площадь участка (Landsize) от типа объекта (Type) \n",
    "# и его региона (Regionname). Чтобы посмотреть несколько статистических параметров, \n",
    "# нужно передать в аргумент aggfunc список из агрегирующих функций. Построим такую сводную таблицу, \n",
    "# где пропущенные значения заменим на 0:\n",
    "melb_df.pivot_table(\n",
    "    values='Landsize',\n",
    "    index='Regionname',\n",
    "    columns='Type',\n",
    "    aggfunc=['median', 'mean'],\n",
    "    fill_value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# МНОГОМЕРНЫЕ СВОДНЫЕ ТАБЛИЦЫ\n",
    "\n",
    "# До этого мы рассматривали, как некоторый статистический показатель может зависеть от двух признаков. \n",
    "# Однако, как уже упоминалось, сводные таблицы позволяют наблюдать зависимость и от большего числа признаков. \n",
    "# Такие сводные таблицы называются многомерными. \n",
    "\n",
    "# Для того чтобы исследовать зависимость от большего числа признаков, \n",
    "# можно передать список признаков в параметр index или параметр columns.\n",
    "\n",
    "# Давайте построим таблицу, в которой по индексам будут располагаться признаки метода продажи (Method) \n",
    "# и типа объекта (Type), по столбцам — наименование региона (Regionname), а на пересечении строк и столбцов \n",
    "# будет стоять медианная цена объекта (Price):\n",
    "melb_df.pivot_table(\n",
    "    values='Price',\n",
    "    index=['Method','Type'],\n",
    "    columns='Regionname',\n",
    "    aggfunc='median',\n",
    "    fill_value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ДОСТУП К ДАННЫМ В СВОДНОЙ ТАБЛИЦЕ\n",
    "\n",
    "# Как получить доступ к данным или произвести фильтрацию в сложной сводной таблице, где есть дополнительные индексы?\n",
    "# Давайте рассмотрим, что собой представляют столбцы сложной сводной таблицы.\n",
    "# Запишем сводную таблицу, которую мы создавали ранее в переменную pivot:\n",
    "pivot = melb_df.pivot_table(\n",
    "    values='Landsize',\n",
    "    index='Regionname',\n",
    "    columns='Type',\n",
    "    aggfunc=['median', 'mean'],\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Выведем её столбцы с помощью атрибута columns:\n",
    "pivot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В результате мы получаем объект MultiIndex. Этот объект хранит в себе шесть комбинаций пар столбцов \n",
    "# (два статистических параметра и три типа здания), то есть есть шесть возможных вариантов обращения к столбцам таблицы.\n",
    "\n",
    "# Мультииндексы раскрываются подобно вложенным словарям — по очереди, как матрёшка. \n",
    "# Чтобы получить доступ к определённому столбцу, вы должны сначала обратиться к столбцу, который находится уровнем выше.\n",
    "\n",
    "# Так, из таблицы pivot мы можем получить средние значения площадей участков для типа здания unit, \n",
    "# просто последовательно обратившись по имени столбцов:\n",
    "print(pivot['mean']['unit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Аналогично производится и фильтрация данных. Например, если нам нужны регионы, \n",
    "# в которых средняя площадь здания для домов типа house меньше их медианной площади, то мы можем найти их следующим образом:\n",
    "mask = pivot['mean']['house'] < pivot['median']['house']\n",
    "filtered_pivot = pivot[mask]\n",
    "print(filtered_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтобы получить индексы отфильтрованной таблицы, можно воспользоваться атрибутом index и обернуть результат в список:\n",
    "print(list(filtered_pivot.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примечание. На самом деле мультииндексные таблицы можно создавать и вручную. Давайте посмотрим на синтаксис данной конструкции:\n",
    "import numpy as np\n",
    "mser = pd.Series(\n",
    "    np.random.rand(8),\n",
    "\tindex=[['white','white','white','blue','blue','red','red','red'], \n",
    "           ['up','down','right','up','down','up','down','left']])\n",
    "print(mser)\n",
    "\n",
    "# В данном примере мы создаём объект Series со вложенными индексами. \n",
    "# Мы передаём в качестве индексов Series вложенный список, где первый список задаёт внешний уровень вложенности, \n",
    "# а второй список — внутренний уровень вложенности. Значения Series — случайные числа от 0 до 1, \n",
    "# сгенерированные функцией np.random.rand() (ваши значения могут отличаться).\n",
    "# Если посмотреть на индексы Series, можно увидеть, что они являются мультииндексами:\n",
    "print(mser.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Аналогично создаются DataFrame со вложенными признаками \n",
    "# (вложенными столбцами) — для этого вложенный список передаётся в параметр columns при инициализации таблицы:\n",
    "mframe = pd.DataFrame(\n",
    "    np.random.randn(16).reshape(4,4),\n",
    "    index=[['white','white','red','red'], ['up','down','up','down']],\n",
    "    columns=[['pen','pen','paper','paper'],[1,2,1,2]]\n",
    ")\n",
    "print(mframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЗАДАНИЕ 4.2\n",
    "# Составьте сводную таблицу, которая показывает зависимость медианной площади (BuildingArea) здания \n",
    "# от типа объекта недвижимости (Type) и количества жилых комнат в доме (Rooms). Для какой комбинации признаков площадь здания наибольшая?\n",
    "# В качестве ответа запишите эту комбинацию (тип здания, число комнат) через запятую, без пробелов.\n",
    "pivot = melb_df.pivot_table(\n",
    "    values='BuildingArea',\n",
    "    index='Type',\n",
    "    columns='Rooms',\n",
    "    aggfunc='median',\n",
    "    fill_value=0\n",
    ")\n",
    "print(pivot)\n",
    "\n",
    "# ЗАДАНИЕ 4.3\n",
    "# Составьте сводную таблицу, которая показывает зависимость средней цены объекта недвижимости (Price) \n",
    "# от риелторского агентства (SellerG) и типа здания (Type).\n",
    "# Во вновь созданной таблице найдите агентство, у которого средняя цена для зданий типа unit максимальна. \n",
    "# В качестве ответа запишите название этого агентств\n",
    "pivot = melb_df.pivot_table(\n",
    "    values='Price',\n",
    "    index='SellerG',\n",
    "    columns='Type',\n",
    "    aggfunc='mean',\n",
    ")\n",
    "max_unit_price = pivot['unit'].max()\n",
    "print(pivot[pivot['unit'] == max_unit_price].index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Объединение DataFrame: concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавить страницу в мои закладки\n",
    "# Следуя нашему плану объединения таблиц, первым делом мы должны склеить таблицы ratings1 и ratings2 по строкам.\n",
    "\n",
    "# Для этого воспользуемся встроенной функцией Pandas concat(), которая позволяет склеивать \n",
    "# (конкатенировать) таблицы как по строкам, так и по столбцам.\n",
    "\n",
    "# Кликните на плашку, чтобы увидеть информацию ↓\n",
    "\n",
    "# Основные параметры функции concat()\n",
    "# Для корректной конкатенации по строкам объединяемые таблицы должны иметь одинаковую структуру — идентичное число и имена столбцов.\n",
    "\n",
    "# Итак, давайте склеим  ratings1 и ratings2 по строкам, так как они имеют одинаковую структуру столбцов. \n",
    "# Для этого передадим их списком в функцию concat(). Помним, что параметр axis по умолчанию равен 0, \n",
    "# объединение происходит по строкам, поэтому не трогаем его. \n",
    "\n",
    "# Примечание. Обратите внимание, что concat является функцией библиотеки, а не методом DataFrame. \n",
    "# Поэтому её вызов осуществляется как pd.concat(...).\n",
    "\n",
    "ratings1 = pd.read_csv('data/ratings1.csv', sep=',')\n",
    "ratings2 = pd.read_csv('data/ratings2.csv', sep=',')\n",
    "dates = pd.read_csv('data/movies.csv', sep=',')\n",
    "movies = pd.read_csv('data/movies.csv', sep=',')\n",
    "\n",
    "ratings = pd.concat([ratings1, ratings2])\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В результате мы увеличили первую таблицу, добавив снизу строки второй таблицы.\n",
    "\n",
    "# На первый взгляд может показаться, что всё прошло успешно, однако если мы посмотрим на индексы последних строк таблицы, \n",
    "# то увидим, что их нумерация не совпадает с количеством строк. Это может привести к некорректному \n",
    "# объединению таблиц по ключевым столбцам на следующем этапе решения нашей задачи.\n",
    "\n",
    "# Это связано с тем, что по умолчанию concat сохраняет первоначальные индексы объединяемых таблиц, \n",
    "# а обе наши таблицы индексировались, начиная от 0. Чтобы создать новые индексы, нужно выставить параметр ignore_index на True:\n",
    "ratings = pd.concat(\n",
    "    [ratings1, ratings2],\n",
    "    ignore_index=True\n",
    ")\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Казалось бы, совсем другое дело! Но это ещё не всё. Давайте узнаем количество строк в таблицах ratings и dates, \n",
    "# ведь нам предстоит вертикально склеить их между собой:\n",
    "\n",
    "print('Число строк в таблице ratings: ', ratings.shape[0])\n",
    "print('Число строк в таблице dates: ', dates.shape[0])\n",
    "print(ratings.shape[0] == dates.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# На самом деле очень просто: при выгрузке данных информация об оценках какого-то\n",
    "# пользователя попала в обе таблицы (ratings1 и ratings2). В результате конкатенации случилось дублирование строк. \n",
    "# В данном примере их легко найти — выведем последнюю строку таблицы ratings1 и первую строку таблицы ratings2:\n",
    "\n",
    "print(ratings1.tail(1))\n",
    "print(ratings2.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтобы очистить таблицу от дублей, мы можем воспользоваться методом DataFrame drop_duplicates(), \n",
    "# который удаляет повторяющиеся строки в таблице. Не забываем обновить индексы после удаления дублей, \n",
    "# выставив параметр ignore_index в методе drop_duplicates() на значение True:\n",
    "ratings = ratings.drop_duplicates(ignore_index=True)\n",
    "print('Число строк в таблице ratings: ', ratings.shape[0])\n",
    "\n",
    "# Наконец, мы можем добавить к нашей таблице с оценками даты их выставления. \n",
    "# Для этого конкатенируем таблицы ratings и dates по столбцам:\n",
    "ratings_dates = pd.concat([ratings, dates], axis=1)\n",
    "print(ratings_dates.tail(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Объединение DataFrame: join, merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У таблиц ratings и movies есть общий столбец movieId, который каждому фильму из таблицы movies ставит \n",
    "в соответствие поставленные ему оценки из таблицы ratings. Мы хотим объединить их в единую структуру согласно этому соответствию. \n",
    "Объединения такого рода часто называют объединением по ключевому столбцу.\n",
    "\n",
    "Однако прежде чем мы перейдём к дальнейшей работе с нашими таблицами о фильмах, мы должны рассмотреть основные типы объединения таблиц.\n",
    "\n",
    "### ТИПЫ ОБЪЕДИНЕНИЙ\n",
    "\n",
    "### inner (внутреннее)\n",
    "\n",
    "При использовании такого типа объединения в результирующей таблице остаются только те записи, которые есть в обеих таблицах.\n",
    "\n",
    "Аналогия в теории множеств\n",
    "Строки, для которых совпадение не было найдено, удаляются.\n",
    "\n",
    "### outer (внешнее)\n",
    "\n",
    "Данный тип делится на три подтипа:\n",
    "\n",
    "full — используется как outer по умолчанию, объединяет все варианты в обеих таблицах.\n",
    "\n",
    "Аналогия в теории множеств\n",
    "left — для всех записей из «левой» таблицы (например, ratings) ведётся поиск соответствий в «правой» (например, movies). \n",
    "В результирующей таблице останутся только те значения, которым были найдены соответствия, то есть только значения из ratings.\n",
    "\n",
    "Аналогия в теории множеств\n",
    "right — аналогично предыдущему, но остаются значения только из «правой» таблицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# МЕТОД ОБЪЕДИНЕНИЯ JOIN\n",
    "\n",
    "# Для объединения двух таблиц по индексам используется метод DataFrame join(). \n",
    "# Однако данный метод можно применить и для того, чтобы объединить таблицы по ключевому столбцу (в нашем случае это movieId).\n",
    "\n",
    "# Если использовать метод join() «в лоб» (без указания ключевого столбца), то объединение произойдёт, \n",
    "# как и задумано — по индексам двух таблиц согласно установленному типу объединения.\n",
    "\n",
    "# Проверим это, объединив таблицы типом left. Так как в наших таблицах есть одноимённые столбцы, установим один из суффиксов, \n",
    "# чтобы избежать ошибки:\n",
    "\n",
    "joined_false = ratings_dates.join(\n",
    "    movies,\n",
    "    rsuffix='_right',\n",
    "    how='left'\n",
    ")\n",
    "print(joined_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# При объединении таблиц по индексам в результирующую таблицу попали все строки из «левой» таблицы, \n",
    "# а недостающие строки из «правой» были заполнены пропусками. Так работает тип объединения left.\n",
    "\n",
    "# Попробуйте изменить тип объединения, чтобы посмотреть на разницу результирующих таблиц.\n",
    "\n",
    "# Обратите внимание, что в данном случае у нас получилось два столбца, соответствующих идентификатору фильма: \n",
    "# один — из «левой» таблицы (movieId), а другой — из «правой» (movieId_right).\n",
    "\n",
    "# Однако это не тот результат, который мы хотели, ведь мы не получили соответствия фильмов и их рейтингов. \n",
    "# Чтобы совместить таблицы по ключевому столбцу с помощью метода join(), \n",
    "# необходимо использовать ключевой столбец в «правой» таблице в качестве индекса. \n",
    "# Это можно сделать с помощью метода set_index(). Также необходимо указать название ключа в параметре on.\n",
    "\n",
    "joined = ratings_dates.join(\n",
    "    movies.set_index('movieId'),\n",
    "    on='movieId',\n",
    "    how='left'\n",
    ")\n",
    "print(joined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# МЕТОД ОБЪЕДИНЕНИЯ MERGE\n",
    "\n",
    "# Аналогично предыдущему, метод merge() предназначен для слияния двух таблиц по ключевым столбцам или по индексам. \n",
    "# Однако, в отличие от join(), метод merge() предлагает более гибкий способ управления объединением, \n",
    "# благодаря чему является более популярным.\n",
    "\n",
    "# Кликните на плашку, чтобы увидеть информацию ↓\n",
    "\n",
    "# Основные параметры метода merge()\n",
    "# Метод merge() в первую очередь предназначен для слияния таблиц по заданным ключам, поэтому он не требует \n",
    "# установки ключевых столбцов в качестве индекса присоединяемой таблицы. Кроме того, данный метод позволяет \n",
    "# объединять даже таблицы с разноимёнными ключами. Таким образом, merge() проще в использовании и более многофункционален, \n",
    "# чем схожие методы.\n",
    "\n",
    "# Посмотрим на метод merge() в действии. Произведём слияние наших таблиц и получим ту же таблицу, что и ранее:\n",
    "merged = ratings_dates.merge(\n",
    "    movies,\n",
    "    on='movieId',\n",
    "    how='left'\n",
    ")\n",
    "print(merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим, что число строк в таблице ratings_dates совпадает с числом строк в результирующей таблице merged:\n",
    "print('Число строк в таблице ratings_dates: ', ratings_dates.shape[0])\n",
    "print('Число строк в таблице merged: ', merged.shape[0])\n",
    "print(ratings_dates.shape[0] == merged.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОСОБЕННОСТИ ИСПОЛЬЗОВАНИЯ MERGE()\n",
    "\n",
    "# Возникает вопрос: почему мы выбрали тип объединения left, а не full, например?\n",
    "\n",
    "# Найти ответ нам поможет пример. Объединим ratings_dates с movies по ключевому столбцу movieId, \n",
    "# но с параметром how='outer' (full outer) и выведем размер таблицы, а также её «хвост»:\n",
    "merged2 = ratings_dates.merge(\n",
    "    movies,\n",
    "    on='movieId',\n",
    "    how='outer'\n",
    ")\n",
    "print('Число строк в таблице merged2: ', merged2.shape[0])\n",
    "print(merged2.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оказывается, в таблице movies содержались фильмы, которым ещё не были выставлены оценки. \n",
    "# В результате объединения типом full outer информация о фильмах перенеслась из таблицы movies в результирующую таблицу. \n",
    "# Однако, поскольку оценки фильмам ещё не были выставлены, соответствующие столбцы таблицы ratings_dates заполнились пропусками (NaN). \n",
    "# Такие фильмы были записаны в конец таблицы.\n",
    "\n",
    "# Важно! Учитывайте такие нюансы при работе с несколькими таблицами и всегда проверяйте результат объединения.\n",
    "\n",
    "# Метод merge() с внешним (outer) типом объединения может использоваться как аналог метода concat() \n",
    "# при объединении таблиц с одинаковой структурой (одинаковые количество и названия столбцов) по строкам. \n",
    "# В таком случае все одноимённые столбцы таблиц будут считаться ключевыми.\n",
    "\n",
    "# Рассмотрим пример: объединим таблицы ratings1 и ratings2, как мы уже делали раньше, но теперь используем метод merge():\n",
    "merge_ratings = ratings1.merge(ratings2, how='outer')\n",
    "print('Число строк в таблице merge_ratings: ', merge_ratings.shape[0])\n",
    "print(merge_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЗАДАНИЕ 7.5\n",
    "# 1. Сформируйте DataFrame merged, в котором в результате объединения purchase_df и items_df останутся модели, \n",
    "# которые учтены на складе и имели продажи.\n",
    "# 2. Из таблицы merged найдите суммарную выручку, которую можно было бы получить от продажи всех товаров, \n",
    "# которые учтены на складе и имели продажи. Результат занесите в переменную income.\n",
    "items_df = pd.DataFrame({\n",
    "'item_id': [417283, 849734, 132223, 573943, 19475, 3294095, 382043, 302948, 100132, 312394], \n",
    "'vendor': ['Samsung', 'LG', 'Apple', 'Apple', 'LG', 'Apple', 'Samsung', 'Samsung', 'LG', 'ZTE'],\n",
    "'stock_count': [54, 33, 122, 18, 102, 43, 77, 143, 60, 19]\n",
    "})\n",
    "\n",
    "purchase_df = pd.DataFrame({\n",
    "    'purchase_id': [101, 101, 101, 112, 121, 145, 145, 145, 145, 221],\n",
    "    'item_id': [417283, 849734, 132223, 573943, 19475, 3294095, 382043, 302948, 103845, 100132], \n",
    "    'price': [13900, 5330, 38200, 49990, 9890, 33000, 67500, 34500, 89900, 11400]\n",
    "})\n",
    "\n",
    "merged = items_df.merge(purchase_df, how='inner', on='item_id')\n",
    "income = (merged['price'] * merged['stock_count']).sum()\n",
    "\n",
    "print(f'merged: {merged}')\n",
    "print()\n",
    "print(f'income: {income}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = pd.read_csv('data/orders.csv', sep=',')\n",
    "product_df = pd.read_csv('data/products.csv', sep=',')\n",
    "ratings_movies_df = pd.read_csv('data/ratings_movies.csv', sep=',')\n",
    "\n",
    "\n",
    "#библиотека для регулярных выражений\n",
    "import re \n",
    "def get_year_release(arg):\n",
    "    #находим все слова по шаблону \"(DDDD)\"\n",
    "    candidates = re.findall(r'\\(\\d{4}\\)', arg) \n",
    "    # проверяем число вхождений\n",
    "    if len(candidates) > 0:\n",
    "        #если число вхождений больше 0,\n",
    "\t#очищаем строку от знаков \"(\" и \")\"\n",
    "        year = candidates[0].replace('(', '')\n",
    "        year = year.replace(')', '')\n",
    "        return int(year)\n",
    "    else:\n",
    "        #если год не указан, возвращаем None\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# ЗАДАНИЕ 8.1\n",
    "# Создайте в таблице новый признак year_release, который соответствует году выпуска фильма.\n",
    "# У скольких фильмов не указан год их выпуска?\n",
    "joined['year_release'] = joined['title'].apply(get_year_release)\n",
    "joined.info()\n",
    "\n",
    "# ЗАДАНИЕ 8.2\n",
    "# Какой фильм, выпущенный в 1999 году, получил наименьшую среднюю оценку зрителей?\n",
    "# В качестве ответа запишите название этого фильма без указания года его выпуска.\n",
    "mask = joined['year_release'] == 1999\n",
    "joined[mask].groupby('title')['rating'].mean().sort_values()\n",
    "\n",
    "# ЗАДАНИЕ 8.3\n",
    "# Какое сочетание жанров фильмов (genres), выпущенных в 2010 году, получило наименьшую среднюю оценку (rating)?\n",
    "mask = joined['year_release'] == 2010\n",
    "joined[mask].groupby('genres')['rating'].mean().sort_values()\n",
    "\n",
    "# ЗАДАНИЕ 8.4\n",
    "# Какой пользователь (userId) посмотрел наибольшее количество различных (уникальных) жанров (genres) фильмов? \n",
    "joined.groupby('userId')['genres'].nunique().sort_values(ascending=False)\n",
    "\n",
    "# ЗАДАНИЕ 8.5\n",
    "# Найдите пользователя, который выставил наименьшее количество оценок, но его средняя оценка фильмам наибольшая.\n",
    "joined.groupby('userId')['rating'].agg(\n",
    "    ['count', 'mean']\n",
    ").sort_values(['count', 'mean'], ascending=[True, False])\n",
    "\n",
    "# ЗАДАНИЕ 8.6\n",
    "# Найдите сочетание жанров (genres) за 2018 году, которое имеет наибольший средний рейтинг (среднее по столбцу rating), \n",
    "# и при этом число выставленных ему оценок (количество значений в столбце rating) больше 10.\n",
    "mask = joined['year_release'] == 2018\n",
    "grouped = joined[mask].groupby('genres')['rating'].agg(\n",
    "    ['mean', 'count']\n",
    ")\n",
    "grouped[grouped['count']>10].sort_values(\n",
    "    by='mean',\n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "# ЗАДАНИЕ 8.7\n",
    "# Добавьте в таблицу новый признак year_rating — год выставления оценки. Создайте сводную таблицу, \n",
    "# которая иллюстрирует зависимость среднего рейтинга фильма от года выставления оценки и жанра. \n",
    "# Выберите верные варианты ответа, исходя из построенной таблицы:\n",
    "joined['date'] = pd.to_datetime(joined['date'])\n",
    "joined['year_rating'] = joined['date'].dt.year\n",
    "pivot = joined.pivot_table(\n",
    "    index='year_rating',\n",
    "    columns='genres',\n",
    "    values='rating',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "print(pivot)\n",
    "\n",
    "# ЗАДАНИЕ 8.8\n",
    "# Какой идентификатор (Order ID) имеет заказ, для которого не оказалось информации о товаре?\n",
    "orders_products = orders_df.merge(\n",
    "    product_df, \n",
    "    left_on='ID товара',\n",
    "    right_on='Product_ID',\n",
    "    how='left')\n",
    "orders_products.tail(1)['Order ID']\n",
    "\n",
    "# ЗАДАНИЕ 8.9\n",
    "# На какой товар была произведена отмена?\n",
    "orders_products[orders_products['Отменен'] == 'Да']['Name']\n",
    "\n",
    "# ЗАДАНИЕ 8.10\n",
    "# Какой покупатель принёс наибольшую суммарную прибыль интернет-магазину за указанный период?\n",
    "# В ответ запишите идентификатор этого покупателя (ID Покупателя).\n",
    "# Прибыль состоит только из оплаченных заказов и рассчитывается как количество купленного товара, умноженное на его цену.\n",
    "orders_products['Profit'] = orders_products['Price'] * orders_products['Количество'] \n",
    "orders_products[orders_products['Оплачен'] == 'Да'].groupby('ID Покупателя')['Profit'].sum().sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
