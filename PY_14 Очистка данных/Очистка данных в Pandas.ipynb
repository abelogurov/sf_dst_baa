{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Знакомство с новыми данными: данные о квартирах от Сбера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В этом модуле мы с вами будем работать с данными с самого настоящего соревнования на платформе Kaggle, \n",
    "# инициатором которого стал Сбер. Соревнование проводилось в 2017 году, его призовой фонд составил 25 000 $. \n",
    "# Требования Сбера состояли в построении модели, которая бы прогнозировала цены на жильё в Москве, \n",
    "# опираясь на параметры самого жилья, а также состояние экономики и финансового сектора в стране.\n",
    "\n",
    "# Датасет представляет собой набор данных из таблицы с информацией о параметрах жилья (train.csv). \n",
    "# В ней содержатся 292 признака о состоянии экономики России на момент продажи недвижимости (macro.csv). \n",
    "\n",
    "# Для упрощения техники очистки данных мы будем отрабатывать на урезанном датасете.\n",
    "\n",
    "# Скачать набор данных в формате csv (разделитель — ',') \n",
    "# можно здесь (csv-файл находится в zip-архиве — распакуйте архив, прежде чем продолжать работу!)\n",
    "\n",
    "# Он содержит информацию о 61 признаке. Их значение мы будем объяснять в процессе работы с данными.\n",
    "\n",
    "# Импортируем библиотеки, которые нам понадобятся (pandas для работы с данными, \n",
    "# numpy для математических преобразований, matplotlib и seaborn для визуализации):\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Прочитаем наши данные и выведем первые пять строк таблицы с помощью head(), чтобы убедиться в том, что всё подгрузилось верно:\n",
    "sber_data = pd.read_csv('data/sber_data.csv')\n",
    "sber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Работа с пропусками: как их обнаружить?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавить страницу в мои закладки\n",
    "# Работа с отсутствующими записями в таблице, пожалуй, одна из самых сложных и неоднозначных. \n",
    "# Она же — самая распространённая для реальных данных. С неё мы и начнём!\n",
    "\n",
    "# В pandas пропуски обозначаются специальным символом NaN (Not-a-Number — «не число»). \n",
    "\n",
    "# Мы с вами уже сталкивались с пропусками, как только начали работать с настоящими данными. \n",
    "# Вспомните датасеты о домах в Мельбурне и коронавирусе. \n",
    "\n",
    "# Чем так плохи пропуски, почему так важно их предобработать?\n",
    "\n",
    "# Ответ очень прост: преобладающее большинство моделей машинного обучения не умеют обрабатывать пропуски, \n",
    "# так как они работают только с числами. Если в данных содержится пустая ячейка таблицы, модель выдаст ошибку. \n",
    "\n",
    "# ПРИЧИНЫ ПОЯВЛЕНИЯ ПРОПУСКОВ В ДАННЫХ\n",
    "\n",
    "# Ошибка ввода данных. Как правило, такая ошибка обусловлена человеческим фактором: никто не застрахован \n",
    "# от случайного пропуска графы при заполнении данных.\n",
    "# Ошибка передачи данных. Эта причина на сегодняшний момент возникает довольно редко: \n",
    "# с появлением протоколов проверки выгружаемой информации потерять данные при передаче их по сети становится сложнее, \n",
    "# но вероятность такого события ненулевая.\n",
    "# Намеренное сокрытие информации. Одна из самых распространённых причин, особенно в социологических опросах.\n",
    "# Дело в том, что пользователи/опрашиваемые/клиенты часто скрывают информацию о себе. \n",
    "# Например, люди, занимающие высокие должности, могут быть связаны контрактом о неразглашении своих доходов. \n",
    "# Прямое отсутствие информации. Эта причина очень распространена в данных для рекомендательных систем. \n",
    "# Представьте, что у нас есть таблицы фильмов и пользователей, которые просматривают их и ставят им оценки. \n",
    "# Мы объединяем всю информацию в одну большую сводную таблицу: например, по строкам идут пользователи, \n",
    "# а по столбцам — фильмы. Но вот незадача: у нас нет информации о рейтингах фильмов, которые пользователь ещё не посмотрел. \n",
    "# В таком случае на пересечении строки с именем пользователя и столбца с названием фильма, который он ещё не смотрел, ставится пропуск. \n",
    "# Главное несчастье состоит в том, что 99 % процентов такой таблицы — это сплошной пропуск.\n",
    "# Мошенничество. Очень острая проблема в финансовой сфере, особенно в банковских данных. \n",
    "# Мошенники нередко указывают ложную информацию или не указывают её вовсе.\n",
    "\n",
    "# КАК ОБНАРУЖИТЬ ПРОПУСКИ?\n",
    "# Ранее мы определяли наличие пропусков в данных с помощью метода info(). \n",
    "# Но этот метод не позволяет точно локализовать места пропущенных значений, \n",
    "# он выводит только число непустых значений и предназначен для определения факта наличия пропусков:\n",
    "\n",
    "# Найти пропуски зачастую довольно просто за исключением тех случаев, когда пропуски скрыты.\n",
    "\n",
    "# В библиотеке pandas специально для этого реализован метод isnull(). Этот метод возвращает новый DataFrame, \n",
    "# в ячейках которого стоят булевы значения True и False. True ставится на месте, где ранее находилось значение NaN.\n",
    "# Посмотрим на результат работы метода на нашей таблице:\n",
    "print(sber_data.isnull().tail())\n",
    "\n",
    "# Из таблицы можно увидеть, где были пропущены значения: ячейки со значением True; ячейки, где стоит False, были изначально заполнены.\n",
    "\n",
    "# Как вы сами понимаете, результат метода isnull() — это, мягко говоря, не самый удобный метод поиска пропусков, \n",
    "# однако он является промежуточным этапом других способов, которые мы рассмотрим далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СПИСОК СТОЛБЦОВ С ПРОПУСКАМИ\n",
    "\n",
    "# Первый способ — это вывести на экран названия столбцов, где число пропусков больше 0. \n",
    "# Для этого вычислим средний по столбцам результат метода isnull(). Получим долю пропусков в каждом столбце.\n",
    "# Умножаем на 100 %, находим столбцы, где доля пропусков больше 0, сортируем по убыванию и выводим результат:\n",
    "\n",
    "cols_null_percent = sber_data.isnull().mean() * 100\n",
    "cols_with_null = cols_null_percent[cols_null_percent>0].sort_values(ascending=False)\n",
    "print(cols_with_null)\n",
    "\n",
    "# Итак, можно увидеть, что у нас большое число пропусков (более 47 %) в столбце hospital_beds_raion \n",
    "# (количество больничных коек в округе). \n",
    "\n",
    "# Далее у нас идут столбцы с числом пропусков чуть больше 20 %: \n",
    "\n",
    "# preschool_quota (число мест в детском саду в районе);\n",
    "# school_quota (число мест в школах в районе);\n",
    "# life_sq (жилая площадь здания в квадратных метрах). \n",
    "# Менее одного процента пропусков содержат признаки:\n",
    "\n",
    "# floor (число этажей в доме);\n",
    "# metro_min_walk (время от дома до ближайшего метро пешком в минутах);\n",
    "# metro_km_walk (расстояние до ближайшего метро в километрах);\n",
    "# railroad_station_walk_km (расстояние до ближайшей ж. д. станции в километрах);\n",
    "# railroad_station_walk_min (время до ближайшей ж. д. станции пешком в минутах). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# СТОЛБЧАТАЯ ДИАГРАММА ПРОПУСКОВ\n",
    "\n",
    "# Иногда столбцов с пропусками становится слишком много и прочитать информацию о них из списка признаков \n",
    "# с цифрами становится слишком затруднительно — цифры начинают сливаться воедино. \n",
    "\n",
    "# Можно воспользоваться столбчатой диаграммой, чтобы визуально оценить соотношение числа пропусков к числу записей. \n",
    "# Самый быстрый способ построить её — использовать метод plot():\n",
    "cols_with_null.plot(\n",
    "    kind='bar',\n",
    "    figsize=(10, 4),\n",
    "    title='Распределение пропусков в данных'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ТЕПЛОВАЯ КАРТА ПРОПУСКОВ \n",
    "\n",
    "# Ещё один распространённый способ визуализации пропусков — тепловая карта. \n",
    "\n",
    "# Её часто используют, когда столбцов с пропусками не так много (меньше 10). \n",
    "# Она позволяет понять не только соотношение пропусков в данных, но и их характерное местоположение в таблице. \n",
    "\n",
    "# Для создания такой тепловой карты можно воспользоваться результатом метода isnull(). \n",
    "# Ячейки таблицы, в которых есть пропуск, будем отмечать жёлтым цветом, а остальные — синим. \n",
    "# Для этого создадим собственную палитру цветов тепловой карты с помощью метода color_pallete() из библиотеки seaborn.\n",
    "colors = ['blue', 'yellow'] \n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "cols = cols_with_null.index\n",
    "ax = sns.heatmap(\n",
    "    sber_data[cols].isnull(),\n",
    "    cmap=sns.color_palette(colors),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Работа с пропусками: методы обработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОТБРАСЫВАНИЕ ЗАПИСЕЙ И ПРИЗНАКОВ\n",
    "\n",
    "# Первая техника — самая простая из всех. Она предполагает простое удаление записей или признаков, в которых содержатся пропуски. \n",
    "\n",
    "# Здесь важно правильно выбрать ось удаления: если мы избавимся от большого числа строк, \n",
    "# то рискуем потерять важные данные, а если мы удалим столбцы, то можем потерять важные признаки.\n",
    "# Прежде всего порассуждаем логически: в столбце hospital_beds_raion более 47 % пропусков. \n",
    "# Если мы будем удалять все строки, в которых этот признак пропущен, мы потеряем почти половину наших данных! \n",
    "\n",
    "# Правильнее будет просто удалить столбец, ведь число мест \n",
    "# в районных больницах — это не самый информативный признак для определения цены квартиры.\n",
    "\n",
    "# А вот если мы удалим весь столбец metro_km_walk, где менее 1 % пропусков, \n",
    "# то потеряем полезный признак при формировании прогноза цены, \n",
    "# ведь расстояние до ближайшего метро — это важный фактор при выборе квартиры. В данном случае лучше будет удалить сами записи.\n",
    "\n",
    "# Специалисты рекомендуют при использовании метода удаления придерживаться следующих правил: \n",
    "# удаляйте столбец, если число пропусков в нем более 30-40 %. В остальных случаях лучше удалять записи.\n",
    "\n",
    "# Для удаления строк и столбцов будем использовать метод dropna(), который позволяет удалять пропуски с тонким подходом к настройке. \n",
    "\n",
    "# Основные параметры метода:\n",
    "\n",
    "# axis — ось, по которой производится удаление (по умолчанию 0 — строки).\n",
    "# how — как производится удаление строк (any — если хотя бы в одном из столбцов есть пропуск, стоит по умолчанию; \n",
    "# all — если во всех столбцах есть пропуски). \n",
    "# thresh — порог удаления. Определяет минимальное число непустых значений в строке/столбце, при котором она/он сохраняется. \n",
    "# Например, если мы установим thresh в значение 2, то мы удалим строки, где число пропусков  и более, где  — число признаков (если ).\n",
    "# Предварительно создадим копию исходной таблицы — drop_data, чтобы не повредить её. \n",
    "# Зададимся порогом в 70 %: будем оставлять только те столбцы, в которых 70 и более процентов записей не являются пустыми. \n",
    "# После этого удалим записи, в которых содержится хотя бы один пропуск. \n",
    "# Наконец, выведем информацию о числе пропусков и наслаждаемся нулями.\n",
    "\n",
    "#создаем копию исходной таблицы\n",
    "drop_data = sber_data.copy()\n",
    "#задаем минимальный порог: вычисляем 70% от числа строк\n",
    "thresh = drop_data.shape[0]*0.7\n",
    "#удаляем столбцы, в которых более 30% (100-70) пропусков\n",
    "drop_data = drop_data.dropna(how='any', thresh=thresh, axis=1)\n",
    "#удаляем записи, в которых есть хотя бы 1 пропуск\n",
    "drop_data = drop_data.dropna(how='any', axis=0)\n",
    "#отображаем результирующую долю пропусков\n",
    "drop_data.isnull().mean()\n",
    "# Посмотрим на результирующее число записей:\n",
    "print(drop_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЗАПОЛНЕНИЕ НЕДОСТАЮЩИХ ЗНАЧЕНИЙ КОНСТАНТАМИ\n",
    "\n",
    "# Чтобы дырявая бочка не протекала, вставьте в дырку пробку. \n",
    "# С этой мыслью связан другой способ бороться с пропусками — заполнение константами. \n",
    "\n",
    "# Чаще всего пустые места заполняют средним/медианой/модой для числовых признаков и модальным значением для категориальных признаков.\n",
    "\n",
    "# Вся сложность заключается в выборе метода заполнения. \n",
    "# Важным фактором при выборе метода является распределение признаков с пропусками. Давайте выведем их на экран. \n",
    "\n",
    "# В pandas это можно сделать с помощью метода hist():\n",
    "cols = cols_with_null.index\n",
    "sber_data[cols].hist(figsize=(20, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Итак, рассмотрим несколько рекомендаций.\n",
    "\n",
    "# Для распределений, похожих на логнормальное, где пик близ нуля, а далее наблюдается постепенный спад частоты, \n",
    "# высока вероятность наличия выбросов (о них мы поговорим чуть позже). Математически доказывается, \n",
    "# что среднее очень чувствительно к выбросам, а вот медиана — нет. \n",
    "# Поэтому предпочтительнее использовать медианное значение для таких признаков.\n",
    "# Если признак числовой и дискретный (например, число этажей, школьная квота), то их заполнение средним/медианой является ошибочным, \n",
    "# так как может получиться число, которое не может являться значением этого признака. \n",
    "# Например, количество этажей — целочисленный признак, а расчёт среднего может дать 2.871. \n",
    "# Поэтому такой признак заполняют либо модой, либо округляют до целого числа \n",
    "# (или нужного количества знаков после запятой) среднее/медиану.\n",
    "# Категориальные признаки заполняются либо модальным значением, либо, если вы хотите оставить информацию о пропуске в данных, \n",
    "# значением 'unknown'. На наше счастье, пропусков в категориях у нас нет.\n",
    "# Иногда в данных бывает такой признак, основываясь на котором, можно заполнить пропуски в другом. \n",
    "# Например, в наших данных есть признак full_sq (общая площадь квартиры). Давайте исходить из предположения, \n",
    "# что, если жилая площадь (life_sq) неизвестна, то она будет равна суммарной площади!\n",
    "# Заполнение значений осуществляется с помощью метода fillna(). \n",
    "# Главный параметр метода — value (значение, на которое происходит заполнение данных в столбце).\n",
    "# Если метод вызывается от имени всего DataFrame, то в качестве value можно использовать словарь, \n",
    "# где ключи — названия столбцов таблицы, а значения словаря — заполняющие константы. \n",
    "\n",
    "# Создадим такой словарь, соблюдая рекомендации, приведённые выше, а также копию исходной таблицы. \n",
    "# Произведём операцию заполнения с помощью метода fillna() и удостоверимся, что пропусков в данных больше нет:\n",
    "\n",
    "#создаем копию исходной таблицы\n",
    "fill_data = sber_data.copy()\n",
    "#создаем словарь имя столбца: число(признак) на который надо заменить пропуски\n",
    "values = {\n",
    "    'life_sq': fill_data['full_sq'],\n",
    "    'metro_min_walk': fill_data['metro_min_walk'].median(),\n",
    "    'metro_km_walk': fill_data['metro_km_walk'].median(),\n",
    "    'railroad_station_walk_km': fill_data['railroad_station_walk_km'].median(),\n",
    "    'railroad_station_walk_min': fill_data['railroad_station_walk_min'].median(),\n",
    "    'hospital_beds_raion': fill_data['hospital_beds_raion'].mode()[0],\n",
    "    'preschool_quota': fill_data['preschool_quota'].mode()[0],\n",
    "    'school_quota': fill_data['school_quota'].mode()[0],\n",
    "    'floor': fill_data['floor'].mode()[0]\n",
    "}\n",
    "\n",
    "#заполняем пропуски в соответствии с заявленным словарем\n",
    "fill_data = fill_data.fillna(values)\n",
    "#выводим результирующую долю пропусков\n",
    "fill_data.isnull().mean()\n",
    "# Посмотрим, на то, как изменились распределения наших признаков:\n",
    "cols = cols_with_null.index\n",
    "fill_data[cols].hist(figsize=(20, 8));\n",
    "\n",
    "# Обратите внимание на то, как сильно изменилось распределение для признака hospital_beds_raion. \n",
    "# Это связано с тем, что мы заполнили модальным значением почти 47 % общих данных. \n",
    "# В результате мы кардинально исказили исходное распределение признака, что может плохо сказаться на модели.\n",
    "\n",
    "# Недостаток метода заполнения константой состоит в том, что мы можем «нафантазировать» новые данные, \n",
    "# которые не учитывают истинного распределения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЗАПОЛНЕНИЕ НЕДОСТАЮЩИХ ЗНАЧЕНИЙ КОНСТАНТАМИ С ДОБАВЛЕНИЕМ ИНДИКАТОРА\n",
    "\n",
    "# Если мы используем заполнение пропусков константами, может быть, имеет смысл сказать модели о том, что на этом месте был пропуск? \n",
    "\n",
    "# Давайте добавим к нашим данным признаки-индикаторы, которые будут сигнализировать о том, \n",
    "# что в столбце на определённом месте в таблице был пропуск. Это место в столбце-индикаторе будем помечать как True. \n",
    "\n",
    "# Эта эвристика пытается снизить влияние искажения признака, указав модели на места, где мы «нафантазировали» данные.\n",
    "# Посмотрим на реализацию. Как обычно, создадим копию indicator_data исходной таблицы. \n",
    "# В цикле пройдёмся по столбцам с пропусками и будем добавлять в таблицу новый признак (с припиской \"was_null\"), \n",
    "# который получается из исходного с помощью применения метода isnull(). После чего произведём обычное заполнение пропусков,\n",
    "# которое мы совершали ранее, и выведем на экран число отсутствующих значений в столбце, чтобы убедиться в результате:\n",
    "\n",
    "#создаем копию исходной таблицы\n",
    "indicator_data = sber_data.copy()\n",
    "#в цикле пробегаемся по названиям столбцов с пропусками\n",
    "for col in cols_with_null.index:\n",
    "    #создаем новый признак-индикатор как col_was_null\n",
    "    indicator_data[col + '_was_null'] = indicator_data[col].isnull()\n",
    "#создаем словарь имя столбца: число(признак) на который надо заменить пропуски   \n",
    "values = {\n",
    "    'life_sq': indicator_data['full_sq'],\n",
    "    'metro_min_walk': indicator_data['metro_min_walk'].median(),\n",
    "    'metro_km_walk': indicator_data['metro_km_walk'].median(),\n",
    "    'railroad_station_walk_km': indicator_data['railroad_station_walk_km'].median(),\n",
    "    'railroad_station_walk_min': indicator_data['railroad_station_walk_min'].median(),\n",
    "    'hospital_beds_raion': indicator_data['hospital_beds_raion'].mode()[0],\n",
    "    'preschool_quota': indicator_data['preschool_quota'].mode()[0],\n",
    "    'school_quota': indicator_data['school_quota'].mode()[0],\n",
    "    'floor': indicator_data['floor'].mode()[0]\n",
    "}\n",
    "#заполняем пропуски в соответствии с заявленным словарем\n",
    "indicator_data = indicator_data.fillna(values)\n",
    "#выводим результирующую долю пропусков\n",
    "indicator_data.isnull().mean()\n",
    "\n",
    "# Метод исходит из предположения, что, если дать модели информацию о том, что в ячейке ранее была пустота, \n",
    "# то она будет меньше доверять таким записям и меньше учитывать её в процессе обучения. Иногда такие фишки действительно работают, \n",
    "# иногда не дают эффекта, а иногда и вовсе могут ухудшить результат обучения и затруднить процесс обучения.\n",
    "\n",
    "# Когда мы добавляем признаки-индикаторы, мы повышаем размерность наших данных. Теперь представьте, что столбцов с пропусками сотня, \n",
    "# по паре штук в каждом. \n",
    "\n",
    "# Не нужно знать высшую математику, чтобы понять, что в таком случае мы увеличим размерность исходной \n",
    "# таблицы ещё на сотню и 99 % строк этих столбцов будут заполнены нулями (False). При увеличении размерности \n",
    "# в данных время обучения некоторых моделей растет экспоненциально — увеличив число признаков в два раза, вы увеличите время обучения \n",
    "# в 7.38 раза! И при этом, возможно, даже не получите прироста в качестве. \n",
    "\n",
    "# Более того, существует такое понятие, как проклятие размерности. Когда мы с вами будем обучать модели, \n",
    "# мы будем искать глобальный минимум некоторой функции потерь. Размерность этой функции определяется числом признаков, \n",
    "# на которых мы обучаем алгоритм. \n",
    "\n",
    "# Проклятие размерности гласит, что, увеличивая размерность функции, мы повышаем сложность поиска этого минимума \n",
    "# и рискуем вовсе не найти его! Об этом страшном проклятии мы ещё будем говорить в курсе по ML и даже попробуем его победить. \n",
    "# Но об этом чуть позже.\n",
    "\n",
    "# Однако, несмотря на свои недостатки, этот метод кажется наиболее логичным из предложенных ранее и часто используется в очистке данных.\n",
    "\n",
    "# КОМБИНИРОВАНИЕ МЕТОДОВ\n",
    "\n",
    "# Наверняка вы уже догадались, что необязательно использовать один метод. Вы можете их комбинировать. Например, мы можем:\n",
    "\n",
    "# удалить столбцы, в которых более 30 % пропусков;\n",
    "# удалить записи, в которых более двух пропусков одновременно;\n",
    "# заполнить оставшиеся ячейки константами.\n",
    "# Посмотрим на реализацию такого подхода в коде:\n",
    "\n",
    "#создаём копию исходной таблицы\n",
    "combine_data = sber_data.copy()\n",
    "\n",
    "#отбрасываем столбцы с числом пропусков более 30% (100-70)\n",
    "n = combine_data.shape[0] #число строк в таблице\n",
    "thresh = n*0.7\n",
    "combine_data = combine_data.dropna(how='any', thresh=thresh, axis=1)\n",
    "\n",
    "#отбрасываем строки с числом пропусков более 2 в строке\n",
    "m = combine_data.shape[1] #число признаков после удаления столбцов\n",
    "combine_data = combine_data.dropna(how='any', thresh=m-2, axis=0)\n",
    "\n",
    "#создаём словарь 'имя_столбца': число (признак), на который надо заменить пропуски \n",
    "values = {\n",
    "    'life_sq': combine_data['full_sq'],\n",
    "    'metro_min_walk': combine_data['metro_min_walk'].median(),\n",
    "    'metro_km_walk': combine_data['metro_km_walk'].median(),\n",
    "    'railroad_station_walk_km': combine_data['railroad_station_walk_km'].median(),\n",
    "    'railroad_station_walk_min': combine_data['railroad_station_walk_min'].median(),\n",
    "    'preschool_quota': combine_data['preschool_quota'].mode()[0],\n",
    "    'school_quota': combine_data['school_quota'].mode()[0],\n",
    "    'floor': combine_data['floor'].mode()[0]\n",
    "}\n",
    "#заполняем оставшиеся записи константами в соответствии со словарем values\n",
    "combine_data = combine_data.fillna(values)\n",
    "#выводим результирующую долю пропусков\n",
    "print(combine_data.isnull().mean())\n",
    "\n",
    "# Выведем результирующее число строк и столбцов:\n",
    "print(combine_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЗАДАНИЕ 4.6\n",
    "\n",
    "df = sber_data.copy()\n",
    "thresh = df.shape[0]*0.5\n",
    "df = df.dropna(thresh=thresh, axis=1)\n",
    "thresh2 = df.shape[1] - 2\n",
    "df = df.dropna(thresh=thresh2, axis=0)\n",
    "df = df.fillna({\n",
    "    'metro_min_walk': df['metro_min_walk'].mean(),\n",
    "    'railroad_station_walk_min': df['railroad_station_walk_min'].mean(),\n",
    "    'school_quota': df['school_quota'].mode()[0]})\n",
    "print(df.isnull().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Выбросы: почему появляются и чем опасны?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# МЕТОД РУЧНОГО ПОИСКА И ЗДРАВОГО СМЫСЛА\n",
    "\n",
    "# Это самый трудоёмкий метод, основанный на житейской логике, методе пристального взгляда и небольшом количестве статистики. \n",
    "# Он предполагает поиск невозможных и нелогичных значений в данных.\n",
    "\n",
    "# # Пусть у нас есть признак, по которому мы будем искать выбросы. \n",
    "# Давайте рассчитаем его статистические показатели (минимум, максимум, среднее, квантили) и по ним попробуем определить наличие аномалий.\n",
    "\n",
    "# Сделать это можно с помощью уже знакомого вам метода describe(). \n",
    "# Рассчитаем статистические показатели для признака жилой площади (life_sq).\n",
    "sber_data['life_sq'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Что нам говорит метод describe()? Во-первых, у нас есть квартиры с нулевой жилой площадью. \n",
    "# Во-вторых, в то время как 75-й квантиль равен 43, максимум превышает 7 тысяч квадратных метров (целый дворец, а не квартира!). \n",
    "# Найдём число квартир с нулевой жилой площадью:\n",
    "print(sber_data[sber_data['life_sq'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# А теперь выведем здания с жилой площадью более 7 000 квадратных метров:\n",
    "print(sber_data[sber_data['life_sq'] > 7000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вот он, красавец! Выброс налицо: гигантская жилая площадь (life_sq), да ещё почти в 100 раз превышает общую площадь (full_sq).\n",
    "# Логичен вопрос: а много ли у нас таких квартир, у которых жилая площадь больше, чем суммарная?\n",
    "# Давайте проверим это с помощью фильтрации:\n",
    "outliers = sber_data[sber_data['life_sq'] > sber_data['full_sq']]\n",
    "print(outliers.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Таких квартир оказывается 37 штук. Подобные наблюдения уже не поддаются здравому смыслу — они являются ошибочными, \n",
    "# и от них стоит избавиться. Для этого можно воспользоваться методом drop() и удалить записи по их индексам:\n",
    "cleaned = sber_data.drop(outliers.index, axis=0)\n",
    "print(f'Результирующее число записей: {cleaned.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ещё пример: давайте посмотрим на признак числа этажей (floor).\n",
    "print(sber_data['floor'].describe())\n",
    "\n",
    "# Снова видим подозрительную максимальную отметку в 77 этажей. Проверим все квартиры, которые находятся выше 50 этажей:\n",
    "print(sber_data[sber_data['floor']> 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# МЕТОД МЕЖКВАРТИЛЬНОГО РАЗМАХА (МЕТОД ТЬЮКИ)\n",
    "\n",
    "# Отличным помощником в поиске потенциальных выбросов является визуализация. \n",
    "# Если признак является числовым, то можно построить гистограмму или коробчатую диаграмму, чтобы найти аномалии.\n",
    "\n",
    "# На гистограмме мы можем увидеть потенциальные выбросы как низкие далеко отстоящие от основной группы столбцов «пеньки», \n",
    "# а на коробчатой диаграмме — точки за пределами усов.\n",
    "\n",
    "# Построим гистограмму и коробчатую диаграмму для признака полной площади (full_sq):\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n",
    "histplot = sns.histplot(data=sber_data, x='full_sq', ax=axes[0]);\n",
    "histplot.set_title('Full Square Distribution');\n",
    "boxplot = sns.boxplot(data=sber_data, x='full_sq', ax=axes[1]);\n",
    "boxplot.set_title('Full Square Boxplot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Алгоритм метода:\n",
    "\n",
    "# вычислить 25-ый и 75-ый квантили (первый и третий квартили) — и  для признака, который мы исследуем;\n",
    "# вычислить межквартильное расстояние: ;\n",
    "# вычислить верхнюю и нижнюю границы Тьюки: \n",
    "\n",
    "# найти наблюдения, которые выходят за пределы границ.\n",
    "\n",
    "# В соответствии с этим алгоритмом напишем функцию outliers_iqr(), которая вам может ещё не раз пригодиться в реальных задачах. \n",
    "# Эта функция принимает на вход DataFrame и признак, по которому ищутся выбросы, а затем возвращает потенциальные выбросы, \n",
    "# найденные с помощью метода Тьюки, и очищенный от них датасет.\n",
    "\n",
    "# Квантили вычисляются с помощью метода quantile(). \n",
    "# Потенциальные выбросы определяются при помощи фильтрации данных по условию выхода за пределы верхней или нижней границы.\n",
    "def outliers_iqr(data, feature):\n",
    "    x = data[feature]\n",
    "    quartile_1, quartile_3 = x.quantile(0.25), x.quantile(0.75),\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "    outliers = data[(x<lower_bound) | (x > upper_bound)]\n",
    "    cleaned = data[(x>lower_bound) & (x < upper_bound)]\n",
    "    return outliers, cleaned\n",
    "\n",
    "# Применим эту функцию к таблице sber_data и признаку full_sq, а также выведем размерности результатов:\n",
    "outliers, cleaned = outliers_iqr(sber_data, 'full_sq')\n",
    "print(f'Число выбросов по методу Тьюки: {outliers.shape[0]}')\n",
    "print(f'Результирующее число записей: {cleaned.shape[0]}')\n",
    "\n",
    "# Согласно классическому методу Тьюки, под выбросы у нас попали 963 записи в таблице. \n",
    "# Давайте построим гистограмму и коробчатую диаграмму на новых данных cleaned_sber_data:\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n",
    "histplot = sns.histplot(data=cleaned, x='full_sq', ax=axes[0]);\n",
    "histplot.set_title('Cleaned Full Square Distribution');\n",
    "boxplot = sns.boxplot(data=cleaned, x='full_sq', ax=axes[1]);\n",
    "boxplot.set_title('Cleaned Full Square Boxplot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЗАДАНИЕ 6.1\n",
    "# Давайте немного модифицируем функцию outliers_iqr(). Добавьте в неё параметры left и right, \n",
    "# которые задают число IQR влево и вправо от границ ящика (пусть по умолчанию они равны 1.5). \n",
    "# Функция, как и раньше, должна возвращать потенциальные выбросы и очищенный DataFrame.\n",
    "def outliers_iqr_mod(data, feature, left=1.5, right=1.5):\n",
    "    x = data[feature]\n",
    "    quartile_1, quartile_3 = x.quantile(0.25), x.quantile(0.75),\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * left)\n",
    "    upper_bound = quartile_3 + (iqr * right)\n",
    "    outliers = data[(x<lower_bound) | (x> upper_bound)]\n",
    "    cleaned = data[(x>lower_bound) & (x < upper_bound)]\n",
    "    return outliers, cleaned\n",
    "\n",
    "sber_data = pd.read_csv('data/sber_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# МЕТОД Z-ОТКЛОНЕНИЙ (МЕТОД СИГМ)\n",
    "\n",
    "# Последний метод, который мы рассмотрим, — это метод, основанный на правиле трёх сигм для нормального распределения. \n",
    "\n",
    "# Правило трёх сигм гласит: если распределение данных является нормальным, \n",
    "# то 99,73 % лежат в интервале от , где   (мю) — математическое ожидание (для выборки это среднее значение), \n",
    "# а (сигма) — стандартное отклонение. Наблюдения, которые лежат за пределами этого интервала, будут считаться выбросами.\n",
    "\n",
    "# А что делать, если данные не распределены нормально? \n",
    "\n",
    "# На такой случай есть один трюк. Иногда для распределений, похожих на логнормальное, может помочь логарифмирование.\n",
    "# Оно может привести исходное распределение к подобию нормального. Причем, основание логарифма может быть любым.\n",
    "\n",
    "# Рассмотрим логарифмирование на примере. \n",
    "# Построим две гистограммы признака расстояния до МКАД (mkad_km): первая — в обычном масштабе, \n",
    "# а вторая — в логарифмическом. Логарифмировать будем с помощью функции log() из библиотеки numpy \n",
    "# (натуральный логарифм — логарифм по основанию числа e). Признак имеет среди своих значений 0. \n",
    "# Из математики известно, что логарифма от 0 не существует, поэтому мы прибавляем к нашему признаку 1,\n",
    "# чтобы не логарифмировать нули и не получать предупреждения.\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "#гистограмма исходного признака\n",
    "histplot = sns.histplot(sber_data['mkad_km'], bins=30, ax=axes[0])\n",
    "histplot.set_title('MKAD Km Distribution');\n",
    "\n",
    "#гистограмма в логарифмическом масштабе\n",
    "log_mkad_km= np.log(sber_data['mkad_km'] + 1)\n",
    "histplot = sns.histplot(log_mkad_km , bins=30, ax=axes[1])\n",
    "histplot.set_title('Log MKAD Km Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Давайте реализуем алгоритм метода z-отклонения. Описание алгоритма метода:\n",
    "\n",
    "# вычислить математическое ожидание  (среднее) и стандартное отклонение  признака ;\n",
    "# вычислить нижнюю и верхнюю границу интервала как:\n",
    "# найти наблюдения, которые выходят за пределы границ.\n",
    "\n",
    "# Напишем функцию outliers_z_score(), которая реализует этот алгоритм. \n",
    "\n",
    "# На вход она принимает DataFrame и признак, по которому ищутся выбросы. \n",
    "# В дополнение добавим в функцию возможность работы в логарифмическом масштабе: для этого введём аргумент log_scale. \n",
    "# Если он равен True, то будем логарифмировать рассматриваемый признак, иначе — оставляем его в исходном виде.\n",
    "\n",
    "# Как и раньше, функция будет возвращать выбросы и очищенные от них данные:\n",
    "def outliers_z_score(data, feature, log_scale=False):\n",
    "    if log_scale:\n",
    "        x = np.log(data[feature]+1)\n",
    "    else:\n",
    "        x = data[feature]\n",
    "    mu = x.mean()\n",
    "    sigma = x.std()\n",
    "    lower_bound = mu - 3 * sigma\n",
    "    upper_bound = mu + 3 * sigma\n",
    "    outliers = data[(x < lower_bound) | (x > upper_bound)]\n",
    "    cleaned = data[(x > lower_bound) & (x < upper_bound)]\n",
    "    return outliers, cleaned\n",
    "\n",
    "# Применим эту функцию к таблице sber_data и признаку mkad_km, а также выведем размерности результатов:\n",
    "outliers, cleaned = outliers_z_score(sber_data, 'mkad_km', log_scale=True)\n",
    "print(f'Число выбросов по методу z-отклонения: {outliers.shape[0]}')\n",
    "print(f'Результирующее число записей: {cleaned.shape[0]}')\n",
    "\n",
    "# Итак, метод z-отклонения нашел нам 33 потенциальных выброса по признаку расстояния до МКАД. \n",
    "# Давайте узнаем, в каких районах (sub_area) представлены эти квартиры:\n",
    "print(outliers['sub_area'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наши потенциальные выбросы — это квартиры из поселений «Роговское» и «Киевский». \n",
    "# Снова обращаемся к силе интернета и «пробиваем» наших подозреваемых. \n",
    "# Эти поселения — самые удалённые районы Московской области; первое из них — это и вовсе граница с Калужской областью. \n",
    "\n",
    "# И тут возникает закономерный вопрос: а стоит ли считать такие наблюдения за выбросы? \n",
    "# Вопрос в действительности не имеет определенного ответа: с одной стороны, метод прямо-таки говорит нам об этом, \n",
    "# а с другой — эти наблюдения имеют право на существование, ведь они являются частью Московской области.\n",
    "\n",
    "# Возможно, мы не учли того факта, что наш логарифм распределения всё-таки не идеально нормален \n",
    "# и в нём присутствует некоторая асимметрия. Возможно, стоит дать некоторое «послабление» на границы интервалов? \n",
    "# Давайте отдельно построим гистограмму прологарифмированного распределения, а также отобразим на гистограмме вертикальные линии, \n",
    "# соответствующие среднему (центру интервала в методе трёх сигм) и границы интервала . \n",
    "# Вертикальные линии можно построить с помощью метода axvline(). Для среднего линия будет обычной, \n",
    "# а для границ интервала — пунктирной (параметр ls ='--'):\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "log_mkad_km = np.log(sber_data['mkad_km'] + 1)\n",
    "histplot = sns.histplot(log_mkad_km, bins=30, ax=ax)\n",
    "histplot.axvline(log_mkad_km.mean(), color='k', lw=2)\n",
    "histplot.axvline(log_mkad_km.mean()+ 3 * log_mkad_km.std(), color='k', ls='--', lw=2)\n",
    "histplot.axvline(log_mkad_km.mean()- 3 * log_mkad_km.std(), color='k', ls='--', lw=2)\n",
    "histplot.set_title('Log MKAD Km Distribution');\n",
    "\n",
    "# Итак, что мы графически построили интервал метода трёх сигм поверх нашего распределения. \n",
    "# Он показывает, какие наблюдения мы берем в интервал, а какие считаем выбросами. \n",
    "# Легко заметить, среднее значение (жирная вертикальная линия) находится левее моды, \n",
    "# это свойство распределений с левосторонней асимметрией. Также видны наблюдения, \n",
    "# которые мы не захватили своим интервалом (небольшой пенек правее верхней границы) — это и есть наши квартиры из из поселений \n",
    "# \"Роговское\" и \"Киевский\". Очевидно, что если немного (меньше чем на одну сигму) \"сдвинуть\" верхнюю границу вправо, \n",
    "# мы захватим эти наблюдения. Давайте сделаем это?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЗАДАНИЕ 6.5\n",
    "# Постройте гистограмму для признака price_doc в логарифмическом масштабе. \n",
    "# А также, добавьте на график линии, отображающие среднее и границы интервала для метода трех сигм. Выберите верные утверждения:\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "log_price = np.log(sber_data['price_doc'])\n",
    "histplot = sns.histplot(log_price, bins=30, ax=ax)\n",
    "histplot.set_title('Log Price Distribution');\n",
    "histplot.axvline(log_price.mean(), color='k', lw=2)\n",
    "histplot.axvline(log_price.mean()+ 3 * log_price.std(), color='k', ls='--', lw=2)\n",
    "histplot.axvline(log_price.mean()- 3 * log_price.std(), color='k', ls='--', lw=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Работа с дубликатами и неинформативными признаками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОБНАРУЖЕНИЕ И ЛИКВИДАЦИЯ ДУБЛИКАТОВ\n",
    "\n",
    "# Способ обнаружения дубликатов зависит от того, что именно вы считаете дубликатом. \n",
    "# Например, за дубликаты можно посчитать записи, у которых совпадают все признаки или их часть.\n",
    "# Если в таблице есть столбец с уникальным идентификатором (id), вы можете попробовать поискать дубликаты по нему: \n",
    "# одинаковые записи могут иметь одинаковый id.\n",
    "\n",
    "# Проверим, есть у нас такие записи: для этого сравним число уникальных значений в столбце id с числом строк. \n",
    "# Число уникальных значений вычислим с помощью метода nunique():\n",
    "sber_data['id'].nunique() == sber_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вроде бы всё в порядке: каждой записи в таблице соответствует свой уникальный идентификатор. \n",
    "# Но это ещё не означает, что в таблице нет дубликатов!\n",
    "\n",
    "# Столбец с id задаёт каждой строке свой уникальный номер, поэтому сама по себе каждая строка является уникальной. \n",
    "# Однако содержимое других столбцов может повторяться.\n",
    "\n",
    "# Чтобы отследить дубликаты, можно воспользоваться методом duplicated(), который возвращает булеву маску для фильтрации. \n",
    "# Для записей, у которых совпадают признаки, переданные методу, он возвращает True, для остальных — False.\n",
    "\n",
    "# У метода есть параметр subset — список признаков, по которым производится поиск дубликатов. \n",
    "# По умолчанию используются все столбцы в DataFrame и ищутся полные дубликаты.\n",
    "\n",
    "# Найдём число полных дубликатов таблице sber_data. Предварительно создадим список столбцов dupl_columns, \n",
    "# по которым будем искать совпадения (все столбцы, не включая id). \n",
    "\n",
    "# Создадим маску дубликатов с помощью метода duplicated() и произведём фильтрацию. \n",
    "# Результат заносим в переменную sber_duplicates. Выведем число строк в результирующем DataFrame:\n",
    "dupl_columns = list(sber_data.columns)\n",
    "dupl_columns.remove('id')\n",
    "\n",
    "mask = sber_data.duplicated(subset=dupl_columns)\n",
    "sber_duplicates = sber_data[mask]\n",
    "print(f'Число найденных дубликатов: {sber_duplicates.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результирующее число записей: 29909\n"
     ]
    }
   ],
   "source": [
    "# Итак, 562 строки в таблице являются полными копиями других записей. \n",
    "# Ручной поиск совпадающих строк по 30 тысячам записей был бы практически невыполним, \n",
    "# а с помощью pandas мы быстро, а главное, легко обнаружили дублирующиеся данные!\n",
    "\n",
    "# Теперь нам необходимо от них избавиться. Для этого легче всего воспользоваться методом drop_duplicates(),\n",
    "# который удаляет повторяющиеся записи из таблицы. \n",
    "\n",
    "# Создадим новую таблицу sber_dedupped, которая будет версией исходной таблицы, очищенной от полных дубликатов.\n",
    "sber_dedupped = sber_data.drop_duplicates(subset=dupl_columns)\n",
    "print(f'Результирующее число записей: {sber_dedupped.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 100.0% уникальных значений\n",
      "oil_chemistry_raion: 99.03% одинаковых значений\n",
      "railroad_terminal_raion: 96.27% одинаковых значений\n",
      "nuclear_reactor_raion: 97.17% одинаковых значений\n",
      "big_road1_1line: 97.44% одинаковых значений\n",
      "mosque_count_1000: 98.08% одинаковых значений\n"
     ]
    }
   ],
   "source": [
    "# НЕИНФОРМАТИВНЫЕ ПРИЗНАКИ\n",
    "\n",
    "# Отсутствием новой и полезной информации могут «похвастаться» не только отдельные записи в таблице, но и целые признаки.\n",
    "\n",
    "# Неинформативными называются признаки, в которых большая часть строк содержит одинаковые значения \n",
    "# (например, пол клиентов в мужском барбершопе), либо наоборот — признак, в котором для большинства записей значения уникальны\n",
    "# (например, номер телефона клиента). \n",
    "\n",
    "# Мы уже сталкивались с неинформативными признаками. Вспомните датасет о домах в Мельбурне, \n",
    "# где был признак адреса дома с уникальным значением для каждого дома. \n",
    "\n",
    "# ОБНАРУЖЕНИЕ И ЛИКВИДАЦИЯ НЕИНФОРМАТИВНЫХ ПРИЗНАКОВ\n",
    "\n",
    "# Чтобы считать признак неинформативным, прежде всего нужно задать какой-то определённый порог. \n",
    "# Например, часто используют пороги в 0.95 и 0.99. Это означает: признак неинформативен, \n",
    "# если в нем 95 % (99 %) одинаковых значений или же 95 % (99 %) данных полностью уникальны.\n",
    "\n",
    "# К сожалению, в pandas пока нет волшебной палочки, которая мгновенно бы выдавала список столбцов, \n",
    "# обладающих низкой информативностью. Однако процедура их поиска легко реализуется вручную.\n",
    "\n",
    "# Разберём алгоритм:\n",
    "# Создаём пустой список low_information_cols, куда будем добавлять названия признаков, которые мы посчитаем неинформативными.\n",
    "\n",
    "# В цикле пройдёмся по всем именам столбцов в таблице и для каждого будем совершать следующие действия:\n",
    "# рассчитаем top_freq — наибольшую относительную частоту с помощью метода value_counts() \n",
    "# с параметром normalize=True. Метод вернёт долю от общих данных, которую занимает каждое уникальное значение в признаке.\n",
    "\n",
    "#список неинформативных признаков\n",
    "low_information_cols = [] \n",
    "\n",
    "#цикл по всем столбцам\n",
    "for col in sber_data.columns:\n",
    "    #наибольшая относительная частота в признаке\n",
    "    top_freq = sber_data[col].value_counts(normalize=True).max()\n",
    "    #доля уникальных значений от размера признака\n",
    "    nunique_ratio = sber_data[col].nunique() / sber_data[col].count()\n",
    "    # сравниваем наибольшую частоту с порогом\n",
    "    if top_freq > 0.95:\n",
    "        low_information_cols.append(col)\n",
    "        print(f'{col}: {round(top_freq*100, 2)}% одинаковых значений')\n",
    "    # сравниваем долю уникальных значений с порогом\n",
    "    if nunique_ratio > 0.95:\n",
    "        low_information_cols.append(col)\n",
    "        print(f'{col}: {round(nunique_ratio*100, 2)}% уникальных значений')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результирующее число признаков: 55\n"
     ]
    }
   ],
   "source": [
    "# Итак, мы нашли шесть неинформативных признаков. Теперь можно удалить их с помощью метода drop(),\n",
    "# передав результирующий список в его аргументы.\n",
    "information_sber_data = sber_data.drop(low_information_cols, axis=1)\n",
    "print(f'Результирующее число признаков: {information_sber_data.shape[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
